{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Model Training - Drug-Target Interaction Prediction\n",
    "\n",
    "This notebook trains GNN models for DTI prediction using the preprocessed molecular graphs and protein features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "import sys\n",
    "sys.path.append('../models')\n",
    "from gnn_models import get_model\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = Path('../data')\n",
    "PROCESSED_DATA = DATA_DIR / 'processed'\n",
    "RAW_DATA = DATA_DIR / 'raw'\n",
    "MODEL_DIR = Path('../models/checkpoints')\n",
    "MODEL_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DAVIS: 68 drugs, 442 proteins\n",
      "Loaded KIBA: 2111 drugs, 229 proteins\n",
      "DAVIS affinity matrix shape: (68, 442)\n",
      "KIBA affinity matrix shape: (2111, 229)\n"
     ]
    }
   ],
   "source": [
    "# Load molecular graphs\n",
    "with open(PROCESSED_DATA / 'molecular_graphs' / 'davis_graphs.pkl', 'rb') as f:\n",
    "    davis_mol_graphs = pickle.load(f)\n",
    "\n",
    "with open(PROCESSED_DATA / 'molecular_graphs' / 'kiba_graphs.pkl', 'rb') as f:\n",
    "    kiba_mol_graphs = pickle.load(f)\n",
    "\n",
    "# Load protein features\n",
    "with open(PROCESSED_DATA / 'protein_features' / 'davis_protein_fixed.pkl', 'rb') as f:\n",
    "    davis_protein_features = pickle.load(f)\n",
    "\n",
    "with open(PROCESSED_DATA / 'protein_features' / 'kiba_protein_fixed.pkl', 'rb') as f:\n",
    "    kiba_protein_features = pickle.load(f)\n",
    "\n",
    "# Load affinity data\n",
    "with open(RAW_DATA / 'davis' / 'Y', 'rb') as f:\n",
    "    davis_Y = pickle.load(f, encoding='latin1')\n",
    "\n",
    "with open(RAW_DATA / 'kiba' / 'Y', 'rb') as f:\n",
    "    kiba_Y = pickle.load(f, encoding='latin1')\n",
    "\n",
    "print(f\"Loaded DAVIS: {len(davis_mol_graphs)} drugs, {len(davis_protein_features)} proteins\")\n",
    "print(f\"Loaded KIBA: {len(kiba_mol_graphs)} drugs, {len(kiba_protein_features)} proteins\")\n",
    "print(f\"DAVIS affinity matrix shape: {davis_Y.shape}\")\n",
    "print(f\"KIBA affinity matrix shape: {kiba_Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create DTI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAVIS dataset: 30056 drug-target pairs\n",
      "Train: 24044, Val: 3005, Test: 3007\n"
     ]
    }
   ],
   "source": [
    "class DTIDataset(Dataset):\n",
    "    \"\"\"Drug-Target Interaction dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, mol_graphs, protein_features, affinity_matrix, threshold=30000):\n",
    "        self.mol_graphs = mol_graphs\n",
    "        self.protein_features = protein_features\n",
    "        self.affinity_matrix = affinity_matrix\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Get drug and protein IDs\n",
    "        self.drug_ids = list(mol_graphs.keys())\n",
    "        self.protein_ids = list(protein_features.keys())\n",
    "        \n",
    "        # Create pairs with known interactions\n",
    "        self.pairs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i, drug_id in enumerate(self.drug_ids):\n",
    "            for j, protein_id in enumerate(self.protein_ids):\n",
    "                affinity = affinity_matrix[i, j]\n",
    "                if affinity < threshold:  # Known interaction\n",
    "                    self.pairs.append((drug_id, protein_id))\n",
    "                    # Convert Kd to pKd for better learning\n",
    "                    pkd = -np.log10(affinity / 1e9)\n",
    "                    self.labels.append(pkd)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        drug_id, protein_id = self.pairs[idx]\n",
    "        \n",
    "        # Get molecular graph\n",
    "        mol_graph = self.mol_graphs[drug_id]\n",
    "        \n",
    "        # Get protein features\n",
    "        protein_feat = torch.FloatTensor(self.protein_features[protein_id])\n",
    "        \n",
    "        # Get label\n",
    "        label = torch.FloatTensor([self.labels[idx]])\n",
    "        \n",
    "        return mol_graph, protein_feat, label\n",
    "\n",
    "# Create datasets\n",
    "davis_dataset = DTIDataset(davis_mol_graphs, davis_protein_features, davis_Y)\n",
    "print(f\"DAVIS dataset: {len(davis_dataset)} drug-target pairs\")\n",
    "\n",
    "# Split into train/val/test\n",
    "train_size = int(0.8 * len(davis_dataset))\n",
    "val_size = int(0.1 * len(davis_dataset))\n",
    "test_size = len(davis_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    davis_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data loaders with batch size 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_131846/256323149.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler('cuda')\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for DTI data\"\"\"\n",
    "    graphs, proteins, labels = zip(*batch)\n",
    "    \n",
    "    # Batch molecular graphs\n",
    "    batched_graph = Batch.from_data_list(graphs)\n",
    "    \n",
    "    # Stack protein features and labels\n",
    "    batched_proteins = torch.stack(proteins)\n",
    "    batched_labels = torch.stack(labels)\n",
    "    \n",
    "    return batched_graph, batched_proteins, batched_labels\n",
    "\n",
    "# Create data loaders\n",
    "scaler = GradScaler('cuda')\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Created data loaders with batch size {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features: 158\n",
      "Protein features: 10\n",
      "GCN - Total params: 129,537, Trainable: 129,537\n",
      "GAT - Total params: 1,427,713, Trainable: 1,427,713\n",
      "GIN - Total params: 179,073, Trainable: 179,073\n"
     ]
    }
   ],
   "source": [
    "# Get feature dimensions\n",
    "sample_graph = next(iter(davis_mol_graphs.values()))\n",
    "num_node_features = sample_graph.x.shape[1]\n",
    "protein_feature_dim = next(iter(davis_protein_features.values())).shape[0]\n",
    "\n",
    "print(f\"Node features: {num_node_features}\")\n",
    "print(f\"Protein features: {protein_feature_dim}\")\n",
    "\n",
    "# Initialize models\n",
    "models = {}\n",
    "model_names = ['gcn', 'gat', 'gin']\n",
    "\n",
    "for model_name in model_names:\n",
    "    model = get_model(\n",
    "        model_name,\n",
    "        num_features=num_node_features,\n",
    "        hidden_dim=128,\n",
    "        num_layers=3,\n",
    "        dropout=0.2,\n",
    "        protein_dim=protein_feature_dim\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    models[model_name] = model\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{model_name.upper()} - Total params: {total_params:,}, Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        graphs, proteins, labels = batch\n",
    "\n",
    "        proteins = proteins.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Explicitly move each graph component\n",
    "        x = graphs.x.to(device)\n",
    "        edge_index = graphs.edge_index.to(device)\n",
    "        batch = graphs.batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x, edge_index, batch, proteins)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(labels)\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            graphs, proteins, labels = batch\n",
    "            proteins = proteins.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Explicitly move each graph component\n",
    "            x = graphs.x.to(device)\n",
    "            edge_index = graphs.edge_index.to(device)\n",
    "            batch = graphs.batch.to(device)\n",
    "            \n",
    "            outputs = model(x, edge_index, batch, proteins)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * len(labels)\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    all_labels = np.array(all_labels).flatten()\n",
    "    \n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    mae = mean_absolute_error(all_labels, all_preds)\n",
    "    r2 = r2_score(all_labels, all_preds)\n",
    "    pearson_corr = pearsonr(all_labels, all_preds)[0]\n",
    "    spearman_corr = spearmanr(all_labels, all_preds)[0]\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': total_loss / len(loader.dataset),\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': np.sqrt(mse),\n",
    "        'r2': r2,\n",
    "        'pearson': pearson_corr,\n",
    "        'spearman': spearman_corr\n",
    "    }\n",
    "    \n",
    "    return metrics, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training GCN model\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/188 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Not compiled with CUDA support",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraphs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraphs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraphs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproteins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/dti-gnn/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dti-gnn/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Bureau/DeepL_stuff/Graph_GNN_Pytorch_PDB/notebooks/../models/gnn_models.py:55\u001b[0m, in \u001b[0;36mGCN_DTI.forward\u001b[0;34m(self, x, edge_index, batch, protein_features)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Graph-level readout\u001b[39;00m\n\u001b[1;32m     54\u001b[0m x_mean \u001b[38;5;241m=\u001b[39m global_mean_pool(x, batch)\n\u001b[0;32m---> 55\u001b[0m x_max \u001b[38;5;241m=\u001b[39m \u001b[43mglobal_max_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m x_sum \u001b[38;5;241m=\u001b[39m global_add_pool(x, batch)\n\u001b[1;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_mean, x_max, x_sum], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dti-gnn/lib/python3.10/site-packages/torch_geometric/nn/pool/glob.py:92\u001b[0m, in \u001b[0;36mglobal_max_pool\u001b[0;34m(x, batch, size)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dti-gnn/lib/python3.10/site-packages/torch_geometric/utils/_scatter.py:119\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m    112\u001b[0m         out \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(size)\u001b[38;5;241m.\u001b[39mscatter_reduce_(\n\u001b[1;32m    113\u001b[0m             dim, index, fill, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduce[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    114\u001b[0m             include_self\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mscatter_reduce_(dim, index, src,\n\u001b[1;32m    116\u001b[0m                                    reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduce[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    117\u001b[0m                                    include_self\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_scatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# For \"mul\" reduction, we prefer `scatter_reduce_` on CPU:\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dti-gnn/lib/python3.10/site-packages/torch_scatter/scatter.py:160\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scatter_min(src, index, dim, out, dim_size)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter_max\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dti-gnn/lib/python3.10/site-packages/torch_scatter/scatter.py:72\u001b[0m, in \u001b[0;36mscatter_max\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscatter_max\u001b[39m(\n\u001b[1;32m     69\u001b[0m         src: torch\u001b[38;5;241m.\u001b[39mTensor, index: torch\u001b[38;5;241m.\u001b[39mTensor, dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     70\u001b[0m         out: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m         dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_scatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_max\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dti-gnn/lib/python3.10/site-packages/torch/_ops.py:1243\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Not compiled with CUDA support"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train each model\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"Training {model_name.upper()} model\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        val_loss = val_metrics['loss']\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), MODEL_DIR / f'{model_name}_best.pt')\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"  Val RMSE: {val_metrics['rmse']:.4f}\")\n",
    "            print(f\"  Val Pearson: {val_metrics['pearson']:.4f}\")\n",
    "    \n",
    "    # Load best model and test\n",
    "    model.load_state_dict(torch.load(MODEL_DIR / f'{model_name}_best.pt'))\n",
    "    test_metrics, test_preds, test_labels = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'test_metrics': test_metrics,\n",
    "        'test_preds': test_preds,\n",
    "        'test_labels': test_labels\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTest Results for {model_name.upper()}:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, (model_name, res) in enumerate(results.items()):\n",
    "    ax = axes[i]\n",
    "    ax.plot(res['train_losses'], label='Train', alpha=0.7)\n",
    "    ax.plot(res['val_losses'], label='Validation', alpha=0.7)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(f'{model_name.upper()} Training Curves')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_metrics = ['rmse', 'mae', 'r2', 'pearson', 'spearman']\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, res in results.items():\n",
    "    row = {'Model': model_name.upper()}\n",
    "    for metric in comparison_metrics:\n",
    "        row[metric.upper()] = res['test_metrics'][metric]\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.set_index('Model')\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Visualize as heatmap\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(comparison_df.T, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'Score'})\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prediction Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for each model\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, (model_name, res) in enumerate(results.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get predictions and labels\n",
    "    preds = res['test_preds']\n",
    "    labels = res['test_labels']\n",
    "    \n",
    "    # Create scatter plot\n",
    "    ax.scatter(labels, preds, alpha=0.5, s=10)\n",
    "    \n",
    "    # Add diagonal line\n",
    "    min_val = min(labels.min(), preds.min())\n",
    "    max_val = max(labels.max(), preds.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5)\n",
    "    \n",
    "    # Add metrics to plot\n",
    "    metrics = res['test_metrics']\n",
    "    text = f\"RMSE: {metrics['rmse']:.3f}\\nPearson: {metrics['pearson']:.3f}\\nR²: {metrics['r2']:.3f}\"\n",
    "    ax.text(0.05, 0.95, text, transform=ax.transAxes, verticalalignment='top',\n",
    "           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlabel('True pKd')\n",
    "    ax.set_ylabel('Predicted pKd')\n",
    "    ax.set_title(f'{model_name.upper()} Predictions')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "results_file = MODEL_DIR / 'training_results.pkl'\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv(MODEL_DIR / 'model_comparison.csv')\n",
    "\n",
    "print(f\"✓ Results saved to {MODEL_DIR}\")\n",
    "print(f\"  - Training results: training_results.pkl\")\n",
    "print(f\"  - Model comparison: model_comparison.csv\")\n",
    "print(f\"  - Model checkpoints: *_best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL TRAINING COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find best model\n",
    "best_model = None\n",
    "best_score = float('inf')\n",
    "\n",
    "for model_name, res in results.items():\n",
    "    rmse = res['test_metrics']['rmse']\n",
    "    if rmse < best_score:\n",
    "        best_score = rmse\n",
    "        best_model = model_name\n",
    "\n",
    "print(f\"\\nBest Model: {best_model.upper()}\")\n",
    "print(f\"Test RMSE: {best_score:.4f}\")\n",
    "print(f\"Test Pearson: {results[best_model]['test_metrics']['pearson']:.4f}\")\n",
    "print(f\"Test R²: {results[best_model]['test_metrics']['r2']:.4f}\")\n",
    "\n",
    "print(f\"\\nModels trained on DAVIS dataset:\")\n",
    "print(f\"  - {len(train_dataset)} training samples\")\n",
    "print(f\"  - {len(val_dataset)} validation samples\")\n",
    "print(f\"  - {len(test_dataset)} test samples\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Fine-tune hyperparameters\")\n",
    "print(f\"  2. Train on KIBA dataset\")\n",
    "print(f\"  3. Explore model interpretability (notebook 05)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dti-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
