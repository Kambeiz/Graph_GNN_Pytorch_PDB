{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Model Training - Drug-Target Interaction Prediction\n\n",
    "This notebook trains GNN models for DTI prediction using the preprocessed molecular graphs and protein features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "import sys\n",
    "sys.path.append('../models')\n",
    "from gnn_models import get_model\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = Path('../data')\n",
    "PROCESSED_DATA = DATA_DIR / 'processed'\n",
    "RAW_DATA = DATA_DIR / 'raw'\n",
    "MODEL_DIR = Path('../models/checkpoints')\n",
    "MODEL_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load molecular graphs\n",
    "with open(PROCESSED_DATA / 'molecular_graphs' / 'davis_graphs.pkl', 'rb') as f:\n",
    "    davis_mol_graphs = pickle.load(f)\n",
    "\n",
    "with open(PROCESSED_DATA / 'molecular_graphs' / 'kiba_graphs.pkl', 'rb') as f:\n",
    "    kiba_mol_graphs = pickle.load(f)\n",
    "\n",
    "# Load protein features\n",
    "with open(PROCESSED_DATA / 'protein_features' / 'davis_protein_fixed.pkl', 'rb') as f:\n",
    "    davis_protein_features = pickle.load(f)\n",
    "\n",
    "with open(PROCESSED_DATA / 'protein_features' / 'kiba_protein_fixed.pkl', 'rb') as f:\n",
    "    kiba_protein_features = pickle.load(f)\n",
    "\n",
    "# Load affinity data\n",
    "with open(RAW_DATA / 'davis' / 'Y', 'rb') as f:\n",
    "    davis_Y = pickle.load(f, encoding='latin1')\n",
    "\n",
    "with open(RAW_DATA / 'kiba' / 'Y', 'rb') as f:\n",
    "    kiba_Y = pickle.load(f, encoding='latin1')\n",
    "\n",
    "print(f\"Loaded DAVIS: {len(davis_mol_graphs)} drugs, {len(davis_protein_features)} proteins\")\n",
    "print(f\"Loaded KIBA: {len(kiba_mol_graphs)} drugs, {len(kiba_protein_features)} proteins\")\n",
    "print(f\"DAVIS affinity matrix shape: {davis_Y.shape}\")\n",
    "print(f\"KIBA affinity matrix shape: {kiba_Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create DTI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIDataset(Dataset):\n",
    "    \"\"\"Drug-Target Interaction dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, mol_graphs, protein_features, affinity_matrix, threshold=30000):\n",
    "        self.mol_graphs = mol_graphs\n",
    "        self.protein_features = protein_features\n",
    "        self.affinity_matrix = affinity_matrix\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Get drug and protein IDs\n",
    "        self.drug_ids = list(mol_graphs.keys())\n",
    "        self.protein_ids = list(protein_features.keys())\n",
    "        \n",
    "        # Create pairs with known interactions\n",
    "        self.pairs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i, drug_id in enumerate(self.drug_ids):\n",
    "            for j, protein_id in enumerate(self.protein_ids):\n",
    "                affinity = affinity_matrix[i, j]\n",
    "                if affinity < threshold:  # Known interaction\n",
    "                    self.pairs.append((drug_id, protein_id))\n",
    "                    # Convert Kd to pKd for better learning\n",
    "                    pkd = -np.log10(affinity / 1e9)\n",
    "                    self.labels.append(pkd)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        drug_id, protein_id = self.pairs[idx]\n",
    "        \n",
    "        # Get molecular graph\n",
    "        mol_graph = self.mol_graphs[drug_id]\n",
    "        \n",
    "        # Get protein features\n",
    "        protein_feat = torch.FloatTensor(self.protein_features[protein_id])\n",
    "        \n",
    "        # Get label\n",
    "        label = torch.FloatTensor([self.labels[idx]])\n",
    "        \n",
    "        return mol_graph, protein_feat, label\n",
    "\n",
    "# Create datasets\n",
    "davis_dataset = DTIDataset(davis_mol_graphs, davis_protein_features, davis_Y)\n",
    "print(f\"DAVIS dataset: {len(davis_dataset)} drug-target pairs\")\n",
    "\n",
    "# Split into train/val/test\n",
    "train_size = int(0.8 * len(davis_dataset))\n",
    "val_size = int(0.1 * len(davis_dataset))\n",
    "test_size = len(davis_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    davis_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for DTI data\"\"\"\n",
    "    graphs, proteins, labels = zip(*batch)\n",
    "    \n",
    "    # Batch molecular graphs\n",
    "    batched_graph = Batch.from_data_list(graphs)\n",
    "    \n",
    "    # Stack protein features and labels\n",
    "    batched_proteins = torch.stack(proteins)\n",
    "    batched_labels = torch.stack(labels)\n",
    "    \n",
    "    return batched_graph, batched_proteins, batched_labels\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Created data loaders with batch size {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature dimensions\n",
    "sample_graph = next(iter(davis_mol_graphs.values()))\n",
    "num_node_features = sample_graph.x.shape[1]\n",
    "protein_feature_dim = next(iter(davis_protein_features.values())).shape[0]\n",
    "\n",
    "print(f\"Node features: {num_node_features}\")\n",
    "print(f\"Protein features: {protein_feature_dim}\")\n",
    "\n",
    "# Initialize models\n",
    "models = {}\n",
    "model_names = ['gcn', 'gat', 'gin']\n",
    "\n",
    "for model_name in model_names:\n",
    "    model = get_model(\n",
    "        model_name,\n",
    "        num_features=num_node_features,\n",
    "        hidden_dim=128,\n",
    "        output_dim=128,\n",
    "        num_layers=3,\n",
    "        dropout=0.2,\n",
    "        protein_dim=protein_feature_dim\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    models[model_name] = model\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{model_name.upper()} - Total params: {total_params:,}, Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        graphs, proteins, labels = batch\n",
    "        graphs = graphs.to(device)\n",
    "        proteins = proteins.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(graphs, proteins)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(labels)\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            graphs, proteins, labels = batch\n",
    "            graphs = graphs.to(device)\n",
    "            proteins = proteins.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(graphs, proteins)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * len(labels)\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    all_labels = np.array(all_labels).flatten()\n",
    "    \n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    mae = mean_absolute_error(all_labels, all_preds)\n",
    "    r2 = r2_score(all_labels, all_preds)\n",
    "    pearson_corr = pearsonr(all_labels, all_preds)[0]\n",
    "    spearman_corr = spearmanr(all_labels, all_preds)[0]\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': total_loss / len(loader.dataset),\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': np.sqrt(mse),\n",
    "        'r2': r2,\n",
    "        'pearson': pearson_corr,\n",
    "        'spearman': spearman_corr\n",
    "    }\n",
    "    \n",
    "    return metrics, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train each model\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"Training {model_name.upper()} model\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        val_loss = val_metrics['loss']\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), MODEL_DIR / f'{model_name}_best.pt')\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"  Val RMSE: {val_metrics['rmse']:.4f}\")\n",
    "            print(f\"  Val Pearson: {val_metrics['pearson']:.4f}\")\n",
    "    \n",
    "    # Load best model and test\n",
    "    model.load_state_dict(torch.load(MODEL_DIR / f'{model_name}_best.pt'))\n",
    "    test_metrics, test_preds, test_labels = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'test_metrics': test_metrics,\n",
    "        'test_preds': test_preds,\n",
    "        'test_labels': test_labels\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTest Results for {model_name.upper()}:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, (model_name, res) in enumerate(results.items()):\n",
    "    ax = axes[i]\n",
    "    ax.plot(res['train_losses'], label='Train', alpha=0.7)\n",
    "    ax.plot(res['val_losses'], label='Validation', alpha=0.7)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(f'{model_name.upper()} Training Curves')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_metrics = ['rmse', 'mae', 'r2', 'pearson', 'spearman']\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, res in results.items():\n",
    "    row = {'Model': model_name.upper()}\n",
    "    for metric in comparison_metrics:\n",
    "        row[metric.upper()] = res['test_metrics'][metric]\n",
    "    comparison_data.append(row)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.set_index('Model')\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Visualize as heatmap\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(comparison_df.T, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'Score'})\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prediction Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for each model\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, (model_name, res) in enumerate(results.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Get predictions and labels\n",
    "    preds = res['test_preds']\n",
    "    labels = res['test_labels']\n",
    "    \n",
    "    # Create scatter plot\n",
    "    ax.scatter(labels, preds, alpha=0.5, s=10)\n",
    "    \n",
    "    # Add diagonal line\n",
    "    min_val = min(labels.min(), preds.min())\n",
    "    max_val = max(labels.max(), preds.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5)\n",
    "    \n",
    "    # Add metrics to plot\n",
    "    metrics = res['test_metrics']\n",
    "    text = f\"RMSE: {metrics['rmse']:.3f}\\nPearson: {metrics['pearson']:.3f}\\nR²: {metrics['r2']:.3f}\"\n",
    "    ax.text(0.05, 0.95, text, transform=ax.transAxes, verticalalignment='top',\n",
    "           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlabel('True pKd')\n",
    "    ax.set_ylabel('Predicted pKd')\n",
    "    ax.set_title(f'{model_name.upper()} Predictions')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "results_file = MODEL_DIR / 'training_results.pkl'\n",
    "with open(results_file, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv(MODEL_DIR / 'model_comparison.csv')\n",
    "\n",
    "print(f\"✓ Results saved to {MODEL_DIR}\")\n",
    "print(f\"  - Training results: training_results.pkl\")\n",
    "print(f\"  - Model comparison: model_comparison.csv\")\n",
    "print(f\"  - Model checkpoints: *_best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL TRAINING COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Find best model\n",
    "best_model = None\n",
    "best_score = float('inf')\n",
    "\n",
    "for model_name, res in results.items():\n",
    "    rmse = res['test_metrics']['rmse']\n",
    "    if rmse < best_score:\n",
    "        best_score = rmse\n",
    "        best_model = model_name\n",
    "\n",
    "print(f\"\\nBest Model: {best_model.upper()}\")\n",
    "print(f\"Test RMSE: {best_score:.4f}\")\n",
    "print(f\"Test Pearson: {results[best_model]['test_metrics']['pearson']:.4f}\")\n",
    "print(f\"Test R²: {results[best_model]['test_metrics']['r2']:.4f}\")\n",
    "\n",
    "print(f\"\\nModels trained on DAVIS dataset:\")\n",
    "print(f\"  - {len(train_dataset)} training samples\")\n",
    "print(f\"  - {len(val_dataset)} validation samples\")\n",
    "print(f\"  - {len(test_dataset)} test samples\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Fine-tune hyperparameters\")\n",
    "print(f\"  2. Train on KIBA dataset\")\n",
    "print(f\"  3. Explore model interpretability (notebook 05)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
